{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"Zshot <p> Zero and Few shot named entity &amp; relationships recognition </p> <p> </p> <p>Documentation: https://ibm.github.io/zshot</p> <p>Source Code: https://github.com/IBM/zshot</p> <p>Paper: https://aclanthology.org/2023.acl-demo.34/</p> <p>Zshot is a highly customisable framework for performing Zero and Few shot named entity recognition.</p> <p>Can be used to perform:</p> <ul> <li>Mentions extraction: Identify globally relevant mentions or mentions relevant for a given domain </li> <li>Wikification: The task of linking textual mentions to entities in Wikipedia</li> <li>Zero and Few Shot named entity recognition: using language description perform NER to generalize to unseen domains</li> <li>Zero and Few Shot named relationship recognition</li> <li>Visualization: Zero-shot NER and RE extraction</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li> <p><code>Python 3.6+</code></p> </li> <li> <p><code>spacy</code> - Zshot rely on Spacy for pipelining and visualization</p> </li> <li><code>torch</code> - PyTorch is required to run pytorch models.</li> <li><code>transformers</code> - Required for pre-trained language models.</li> <li><code>evaluate</code> - Required for evaluation.</li> <li><code>datasets</code> - Required to evaluate over datasets (e.g.: OntoNotes).</li> </ul>"},{"location":"#optional-dependencies","title":"Optional Dependencies","text":"<ul> <li><code>flair</code> - Required if you want to use Flair mentions extractor and for TARS linker and TARS Mentions Extractor.</li> <li><code>blink</code> - Required if you want to use Blink for linking to Wikipedia pages.</li> <li><code>gliner</code> - Required if you want to use GLiNER Linker or GLiNER Mentions Extractor.</li> <li><code>relik</code> - Required if you want to use Relik Linker.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install zshot\n\n---&gt; 100%\n</code></pre>"},{"location":"#examples","title":"Examples","text":"Example Notebook Installation and Visualization Knowledge Extractor Wikification Custom Components Evaluation"},{"location":"#zshot-approach","title":"Zshot Approach","text":"<p>ZShot contains two different components, the mentions extractor and the linker.</p>"},{"location":"#mentions-extractor","title":"Mentions Extractor","text":"<p>The mentions extractor will detect the possible entities (a.k.a. mentions), that will be then linked to a data source (e.g.: Wikidata) by the linker. </p> <p>Currently, there are 7 different mentions extractors supported, SMXM, TARS, GLiNER, 2 based on SpaCy, and 2 that are based on Flair. The two different versions for SpaCy and Flair are similar, one is based on Named Entity Recognition and Classification (NERC) and the other one is based on the linguistics (i.e.: using Part Of the Speech tagging (PoS) and Dependency Parsing(DP)).</p> <p>The NERC approach will use NERC models to detect all the entities that have to be linked. This approach depends on the model that is being used, and the entities the model has been trained on, so depending on the use case and the target entities it may be not the best approach, as the entities may be not recognized by the NERC model and thus won't be linked.</p> <p>The linguistic approach relies on the idea that mentions will usually be a syntagma or a noun. Therefore, this approach detects nouns that are included in a syntagma and that act like objects, subjects, etc. This approach do not depend on the model (although the performance does), but a noun in a text should be always a noun, it doesn't depend on the dataset the model has been trained on.</p>"},{"location":"#linker","title":"Linker","text":"<p>The linker will link the detected entities to a existing set of labels. Some of the linkers, however, are end-to-end, i.e. they don't need the mentions extractor, as they detect and link the entities at the same time.  </p> <p>Again, there are 6 linkers available currently, 4 of them are end-to-end and 2 are not.</p> Linker Name end-to-end Source Code Paper Blink X Source Code Paper GENRE X Source Code Paper SMXM \u2713 Source Code Paper TARS \u2713 Source Code Paper GLINER \u2713 Source Code Paper RELIK \u2713 Source Code Paper"},{"location":"#relations-extractor","title":"Relations Extractor","text":"<p>The relations extractor will extract relations among different entities previously extracted by a linker.. </p> <p>Currently, the is only one Relation Extractor available: </p> <ul> <li>ZS-Bert</li> <li>Paper</li> <li>Source Code</li> </ul>"},{"location":"#knowledge-extractor","title":"Knowledge Extractor","text":"<p>The knowledge extractor will perform at the same time the extraction and classification of named entities and the extraction of relations among them. The pipeline with this component doesn't need any mentions extractor, linker or relation extractor to work.</p> <p>Currently, there are only two Knowledge Extractor available: </p> <ul> <li> <p>KnowGL</p> </li> <li> <p>Rossiello et al. (AAAI 2023)</p> </li> <li> <p>Mihindukulasooriya et al. (ISWC 2022)</p> </li> <li> <p>Relik</p> </li> <li> <p>Paper</p> </li> <li>Source Code</li> </ul>"},{"location":"#how-to-use-it","title":"How to use it","text":"<ul> <li>Install requirements: <code>pip install -r requirements.txt</code></li> <li>Install a spacy pipeline to use it for mentions extraction: <code>python -m spacy download en_core_web_sm</code></li> <li>Create a file <code>main.py</code> with the pipeline configuration and entities definition (Wikipedia abstract are usually a good starting point for descriptions):</li> </ul> <pre><code>import spacy\n\nfrom zshot import PipelineConfig, displacy\nfrom zshot.linker import LinkerRegen\nfrom zshot.mentions_extractor import MentionsExtractorSpacy\nfrom zshot.utils.data_models import Entity\n\nnlp = spacy.load(\"en_core_web_sm\")\nnlp_config = PipelineConfig(\n    mentions_extractor=MentionsExtractorSpacy(),\n    linker=LinkerRegen(),\n    entities=[\n        Entity(name=\"Paris\",\n               description=\"Paris is located in northern central France, in a north-bending arc of the river Seine\"),\n        Entity(name=\"IBM\",\n               description=\"International Business Machines Corporation (IBM) is an American multinational technology corporation headquartered in Armonk, New York\"),\n        Entity(name=\"New York\", description=\"New York is a city in U.S. state\"),\n        Entity(name=\"Florida\", description=\"southeasternmost U.S. state\"),\n        Entity(name=\"American\",\n               description=\"American, something of, from, or related to the United States of America, commonly known as the United States or America\"),\n        Entity(name=\"Chemical formula\",\n               description=\"In chemistry, a chemical formula is a way of presenting information about the chemical proportions of atoms that constitute a particular chemical compound or molecule\"),\n        Entity(name=\"Acetamide\",\n               description=\"Acetamide (systematic name: ethanamide) is an organic compound with the formula CH3CONH2. It is the simplest amide derived from acetic acid. It finds some use as a plasticizer and as an industrial solvent.\"),\n        Entity(name=\"Armonk\",\n               description=\"Armonk is a hamlet and census-designated place (CDP) in the town of North Castle, located in Westchester County, New York, United States.\"),\n        Entity(name=\"Acetic Acid\",\n               description=\"Acetic acid, systematically named ethanoic acid, is an acidic, colourless liquid and organic compound with the chemical formula CH3COOH\"),\n        Entity(name=\"Industrial solvent\",\n               description=\"Acetamide (systematic name: ethanamide) is an organic compound with the formula CH3CONH2. It is the simplest amide derived from acetic acid. It finds some use as a plasticizer and as an industrial solvent.\"),\n    ]\n)\nnlp.add_pipe(\"zshot\", config=nlp_config, last=True)\n\ntext = \"International Business Machines Corporation (IBM) is an American multinational technology corporation\" \\\n       \" headquartered in Armonk, New York, with operations in over 171 countries.\"\n\ndoc = nlp(text)\ndisplacy.serve(doc, style=\"ent\")\n</code></pre>"},{"location":"#run-it","title":"Run it","text":"<p>Run with</p> <pre><code>$ python main.py\n\nUsing the 'ent' visualizer\nServing on http://0.0.0.0:5000 ...\n</code></pre> <p>The script will annotate the text using Zshot and use Displacy for visualising the annotations</p>"},{"location":"#check-it","title":"Check it","text":"<p>Open your browser at http://127.0.0.1:5000 .</p> <p>You will see the annotated sentence:</p> <p></p>"},{"location":"#how-to-create-a-custom-component","title":"How to create a custom component","text":"<p>If you want to implement your own mentions_extractor or linker and use it with ZShot you can do it. To make it easier for the user to implement a new component, some base classes are provided that you have to extend with your code.</p> <p>It is as simple as create a new class extending the base class (<code>MentionsExtractor</code> or <code>Linker</code>). You will have to implement the predict method, which will receive the SpaCy Documents and will return a list of <code>zshot.utils.data_models.Span</code> for each document.</p> <p>This is a simple mentions_extractor that will extract as mentions all words that contain the letter s:</p> <pre><code>from typing import Iterable\nimport spacy\nfrom spacy.tokens import Doc\nfrom zshot import PipelineConfig\nfrom zshot.utils.data_models import Span\nfrom zshot.mentions_extractor import MentionsExtractor\n\nclass SimpleMentionExtractor(MentionsExtractor):\n    def predict(self, docs: Iterable[Doc], batch_size=None):\n        spans = [[Span(tok.idx, tok.idx + len(tok)) for tok in doc if \"s\" in tok.text] for doc in docs]\n        return spans\n\nnew_nlp = spacy.load(\"en_core_web_sm\")\n\nconfig = PipelineConfig(\n    mentions_extractor=SimpleMentionExtractor()\n)\nnew_nlp.add_pipe(\"zshot\", config=config, last=True)\ntext_acetamide = \"CH2O2 is a chemical compound similar to Acetamide used in International Business \" \\\n        \"Machines Corporation (IBM).\"\n\ndoc = new_nlp(text_acetamide)\nprint(doc._.mentions)\n\n&gt;&gt;&gt; [is, similar, used, Business, Machines, materials]\n</code></pre>"},{"location":"#how-to-evaluate-zshot","title":"How to evaluate ZShot","text":"<p>Evaluation is an important process to keep improving the performance of the models, that's why ZShot allows to evaluate the component with two predefined datasets: OntoNotes and MedMentions, in a Zero-Shot version in which the entities of the test and validation splits don't appear in the train set.  </p> <p>The package <code>evaluation</code> contains all the functionalities to evaluate the ZShot components. The main function is <code>zshot.evaluation.zshot_evaluate.evaluate</code>, that will take as input the SpaCy <code>nlp</code> model and the dataset to evaluate. It will return a <code>str</code> containing a table with the results of the evaluation. For instance the evaluation of the TARS linker in ZShot for the Ontonotes validation set would be:</p> <pre><code>import spacy\n\nfrom zshot import PipelineConfig\nfrom zshot.linker import LinkerTARS\nfrom zshot.evaluation.dataset import load_ontonotes_zs\nfrom zshot.evaluation.zshot_evaluate import evaluate, prettify_evaluate_report\nfrom zshot.evaluation.metrics._seqeval._seqeval import Seqeval\n\nontonotes_zs = load_ontonotes_zs('validation')\n\n\nnlp = spacy.blank(\"en\")\nnlp_config = PipelineConfig(\n    linker=LinkerTARS(),\n    entities=ontonotes_zs.entities\n)\n\nnlp.add_pipe(\"zshot\", config=nlp_config, last=True)\n\nevaluation = evaluate(nlp, ontonotes_zs, metric=Seqeval())\nprettify_evaluate_report(evaluation)\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<pre><code>@inproceedings{picco-etal-2023-zshot,\n    title = \"Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction\",\n    author = \"Picco, Gabriele  and\n      Martinez Galindo, Marcos  and\n      Purpura, Alberto  and\n      Fuchs, Leopold  and\n      Lopez, Vanessa  and\n      Hoang, Thanh Lam\",\n    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)\",\n    month = jul,\n    year = \"2023\",\n    address = \"Toronto, Canada\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2023.acl-demo.34\",\n    doi = \"10.18653/v1/2023.acl-demo.34\",\n    pages = \"357--368\",\n    abstract = \"The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.\",\n}\n</code></pre>"},{"location":"blink/","title":"Blink","text":""},{"location":"blink/#blink","title":"BLINK","text":"<p>BLINK is an Entity Linking model released by Facebook that uses Wikipedia as the target knowledge base. The process of linking entities to Wikipedia is also known as Wikification.</p> <p>In a nutshell, BLINK uses a two stages approach for entity linking, based on fine-tuned BERT architectures. In the first stage, BLINK performs retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then examined more carefully with a cross-encoder, that concatenates the mention and entity text. BLINK achieves state-of-the-art results on multiple datasets.</p> <p></p> <p>The BLINK knowledge base (entity library) is based on the 2019/08/01 Wikipedia dump, so the target entities are Wikipedia entities or articles. </p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>Linker</code></p> <p>Blink linker</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <p>Index to use to perform the entity linking. One of: - BlinkIndex.FLAT - BlinkIndex.HNSW</p> <code>FLAT</code>"},{"location":"blink/#zshot.linker.LinkerBlink.entities_list","title":"<code>entities_list</code>  <code>property</code>","text":"<p>Get list of entities</p>"},{"location":"blink/#zshot.linker.LinkerBlink.local_id2wikipedia_id","title":"<code>local_id2wikipedia_id</code>  <code>property</code>","text":"<p>Get the Wikipedia ID from the label predicted</p>"},{"location":"blink/#zshot.linker.LinkerBlink.download_models","title":"<code>download_models()</code>","text":"<p>Download Blink files</p>"},{"location":"blink/#zshot.linker.LinkerBlink.load_models","title":"<code>load_models()</code>","text":"<p>Load models</p>"},{"location":"blink/#zshot.linker.LinkerBlink.local_name2wikipedia_url","title":"<code>local_name2wikipedia_url(label)</code>","text":"<p>Get the Wikipedia URL of the label predicted to perform wikification</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Label to get URL of</p> required <p>Returns:</p> Type Description <code>str</code> <p>URL of the Wikipedia article</p>"},{"location":"blink/#zshot.linker.LinkerBlink.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"entity_linking/","title":"Linker","text":"<p>The linker will link the detected entities to a existing set of labels. Some of the linkers, however, are end-to-end, i.e. they don't need the mentions extractor, as they detect and link the entities at the same time.  </p> <p>There are 6 linkers available currently, 4 of them are end-to-end and 2 are not.</p> Linker Name end-to-end Source Code Paper Blink X Source Code Paper GENRE X Source Code Paper SMXM \u2713 Source Code Paper TARS \u2713 Source Code Paper GLINER \u2713 Source Code Paper RELIK \u2713 Source Code Paper <p>               Bases: <code>ABC</code></p> <p>Linker define a standard interface for entity linking. A Linker may relay on existing extracted mentions or perform end-2-end extraction</p>"},{"location":"entity_linking/#zshot.Linker.entities","title":"<code>entities</code>  <code>property</code>","text":"<p>Entities to link to</p>"},{"location":"entity_linking/#zshot.Linker.link","title":"<code>link(docs, batch_size=None)</code>","text":"<p>Perform the entity linking. Call the predict function and add entities to the Spacy Docs</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description"},{"location":"entity_linking/#zshot.Linker.load_models","title":"<code>load_models()</code>","text":"<p>Load the model</p> <p>Returns:</p> Type Description"},{"location":"entity_linking/#zshot.Linker.predict","title":"<code>predict(docs, batch_size=None)</code>  <code>abstractmethod</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"entity_linking/#zshot.Linker.set_device","title":"<code>set_device(device)</code>","text":"<p>Set the device to use</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, device]</code> required <p>Returns:</p> Type Description"},{"location":"entity_linking/#zshot.Linker.set_kg","title":"<code>set_kg(entities)</code>","text":"<p>Set entities that linker can use</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>Iterator[Entity]</code> <p>The list of entities</p> required"},{"location":"evaluation/","title":"Evaluation","text":"<p>Evaluation is an important process to keep improving the performance of the models, that's why ZShot allows to evaluate the component with two predefined datasets: OntoNotes and MedMentions, in a Zero-Shot version in which the entities of the test and validation splits don't appear in the train set.  </p>"},{"location":"evaluation/#ontonotes","title":"OntoNotes","text":"<p>OntoNotes Release 5.0 is the final release of the OntoNotes project, a collaborative effort between BBN Technologies, the University of Colorado, the University of Pennsylvania and the University of Southern Californias Information Sciences Institute. The goal of the project was to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).</p> <p>In ZShot the version taken from the Huggingface datasets is preprocessed to adapt it to the Zero-Shot version. </p> <p>Link</p>"},{"location":"evaluation/#medmentions","title":"MedMentions","text":"<p>Corpus: The MedMentions corpus consists of 4,392 papers (Titles and Abstracts) randomly selected from among papers released on PubMed in 2016, that were in the biomedical field, published in the English language, and had both a Title and an Abstract.</p> <p>Annotators: We recruited a team of professional annotators with rich experience in biomedical content curation to exhaustively annotate all UMLS\u00ae (2017AA full version) entity mentions in these papers.</p> <p>Annotation quality: We did not collect stringent IAA (Inter-annotator agreement) data. To gain insight on the annotation quality of MedMentions, we randomly selected eight papers from the annotated corpus, containing a total of 469 concepts. Two biologists ('Reviewer') who did not participate in the annotation task then each reviewed four papers. The agreement between Reviewers and Annotators, an estimate of the Precision of the annotations, was 97.3%.</p> <p>In ZShot the data is downloaded from the original repository and preprocessed to convert it into the Zero-Shot version.</p> <p>Link</p>"},{"location":"evaluation/#how-to-evaluate-zshot","title":"How to evaluate ZShot","text":"<p>The package <code>evaluation</code> contains all the functionalities to evaluate the ZShot components. The main function is <code>zshot.evaluation.zshot_evaluate.evaluate</code>, that will take as input the SpaCy <code>nlp</code> model and the dataset to evaluate. It will return a <code>str</code> containing a table with the results of the evaluation. For instance the evaluation of the TARS linker in ZShot for the Ontonotes validation set would be:</p> <pre><code>import spacy\n\nfrom zshot import PipelineConfig\nfrom zshot.linker import LinkerTARS\nfrom zshot.evaluation.dataset import load_ontonotes_zs\nfrom zshot.evaluation.zshot_evaluate import evaluate, prettify_evaluate_report\nfrom zshot.evaluation.metrics._seqeval._seqeval import Seqeval\n\nontonotes_zs = load_ontonotes_zs('validation')\n\nnlp = spacy.blank(\"en\")\nnlp_config = PipelineConfig(\n    linker=LinkerTARS(),\n    entities=ontonotes_zs.entities\n)\n\nnlp.add_pipe(\"zshot\", config=nlp_config, last=True)\n\nevaluation = evaluate(nlp, ontonotes_zs, metric=Seqeval())\nprettify_evaluate_report(evaluation)\n</code></pre>"},{"location":"flair_mentions_extractor/","title":"Flair Mentions Extractor","text":"<p>               Bases: <code>MentionsExtractor</code></p> <p>Flair Mentions extractor</p> <ul> <li>Requires flair package to be installed *</li> </ul> <p>Parameters:</p> Name Type Description Default <code>extractor_type</code> <code>Optional[ExtractorType]</code> <p>Type of extractor to get mentions. One of: - NER: to use Named Entity Recognition model to get the mentions - POS: to get the mentions based on the linguistics</p> <code>NER</code>"},{"location":"flair_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorFlair.load_models","title":"<code>load_models()</code>","text":"<p>Load Flair model to perform the mentions extraction</p>"},{"location":"flair_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorFlair.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Predict mentions in each document</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Documents to get mentions of</p> required <code>batch_size</code> <p>Batch size to use</p> <code>None</code> <p>Returns:</p> Type Description <p>Spans of the mentions</p>"},{"location":"flair_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorFlair.predict_ner_mentions","title":"<code>predict_ner_mentions(docs, batch_size=None)</code>","text":"<p>Predict mentions of docs using NER model</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Documents to get mentions of</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Batch size to use</p> <code>None</code> <p>Returns:</p> Type Description <p>Spans of the mentions</p>"},{"location":"flair_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorFlair.predict_pos_mentions","title":"<code>predict_pos_mentions(docs, batch_size=None)</code>","text":"<p>Predict mentions of docs using POS linguistics</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Documents to get mentions of</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Batch size to use</p> <code>None</code> <p>Returns:</p> Type Description <p>Spans of the mentions</p>"},{"location":"gliner_linker/","title":"GLiNER Linker","text":"<p>GLiNER is a Named Entity Recognition (NER) model capable of identifying any entity type using a bidirectional transformer encoder (BERT-like). It provides a practical alternative to traditional NER models, which are limited to predefined entities, and Large Language Models (LLMs) that, despite their flexibility, are costly and large for resource-constrained scenarios.</p> <p>The GLiNER linker will use the entities specified in the <code>zshot.PipelineConfig</code>, it just uses the names of the entities, it doesn't use the descriptions of the entities.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>Linker</code></p> <p>GLINER linker</p>"},{"location":"gliner_linker/#zshot.linker.LinkerGLINER.is_end2end","title":"<code>is_end2end</code>  <code>property</code>","text":"<p>GLINER is end2end model</p>"},{"location":"gliner_linker/#zshot.linker.LinkerGLINER.load_models","title":"<code>load_models()</code>","text":"<p>Load GLINER model</p>"},{"location":"gliner_linker/#zshot.linker.LinkerGLINER.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"gliner_mentions_extractor/","title":"GLiNER Mentions Extractor","text":"<p>GLiNER is a Named Entity Recognition (NER) model capable of identifying any entity type using a bidirectional transformer encoder (BERT-like). It provides a practical alternative to traditional NER models, which are limited to predefined entities, and Large Language Models (LLMs) that, despite their flexibility, are costly and large for resource-constrained scenarios.</p> <p>The GLiNER mentions extractor will use the mentions specified in the <code>zshot.PipelineConfig</code>, it just uses the names of the mentions, it doesn't use the descriptions of the mentions.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>MentionsExtractor</code></p> <p>GLiNER Mentions Extractor</p>"},{"location":"gliner_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorGLINER.load_models","title":"<code>load_models()</code>","text":"<p>Load GLINER model</p>"},{"location":"gliner_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorGLINER.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"knowgl_knowledge_extractor/","title":"KnowGL Knowledge Extractor","text":"<p>The knowgl-large model is trained by combining Wikidata with an extended version of the training data in the REBEL dataset. Given a sentence, KnowGL generates triple(s) in the following format:  <pre><code>[(subject mention # subject label # subject type) | relation label | (object mention # object label # object type)]\n</code></pre> If there are more than one triples generated, they are separated by $ in the output. The model achieves state-of-the-art results for relation extraction on the REBEL dataset. The generated labels (for the subject, relation, and object) and their types can be directly mapped to Wikidata IDs associated with them.</p> <p>This <code>KnowledgeExtractor</code> does not use any entity/relation pre-defined.</p> <ul> <li>Paper Rossiello et al. (AAAI 2023)</li> <li>Paper Mihindukulasooriya et al. (ISWC 2022)</li> <li>Original Model</li> </ul> <p>               Bases: <code>KnowledgeExtractor</code></p> <p>Instantiate the KnowGL Knowledge Extractor</p>"},{"location":"knowgl_knowledge_extractor/#zshot.knowledge_extractor.KnowGL.load_models","title":"<code>load_models()</code>","text":"<p>Load KnowGL model</p>"},{"location":"knowgl_knowledge_extractor/#zshot.knowledge_extractor.KnowGL.parse_result","title":"<code>parse_result(result, doc, encodings)</code>","text":"<p>Parse the text result into a list of triples</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>str</code> <p>Text generate by the KnowGL model</p> required <code>doc</code> <code>Doc</code> <p>Spacy doc</p> required <code>encodings</code> <code>Encoding</code> <p>Encodings result of the tokenization</p> required <p>Returns:</p> Type Description <code>List[Tuple[Span, RelationSpan, Span]]</code> <p>List of triples (subject, relation, object)</p>"},{"location":"knowgl_knowledge_extractor/#zshot.knowledge_extractor.KnowGL.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Extract triples from docs</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Spacy Docs to process</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>Batch size for processing</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[Span, RelationSpan, Span]]]</code> <p>Triples (subject, relation, object) extracted for each document</p>"},{"location":"knowledge_extractor/","title":"Knowledge Extractor","text":"<p>The knowledge extractor will perform at the same time the extraction and classification of named entities and the extraction of relations among them. </p> <p>Currently, the are only two Knowledge Extractor available: KnowGL and ReLiK</p> <p>               Bases: <code>ABC</code></p> <p>Instantiate the Knowledge Extractor</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[Union[str, device]]</code> <p>Device to be used for computation</p> <code>None</code>"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.__hash__","title":"<code>__hash__()</code>","text":"<p>Get hash representation of the component</p>"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.extract_knowledge","title":"<code>extract_knowledge(docs, batch_size=None)</code>","text":"<p>Perform the relations extraction. Call the predict function and add the mentions to the Spacy Doc</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.from_disk","title":"<code>from_disk(path, exclude=())</code>  <code>classmethod</code>","text":"<p>Load component from disk</p>"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.load_models","title":"<code>load_models()</code>","text":"<p>Load the model</p> <p>Returns:</p> Type Description"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.parse_triples","title":"<code>parse_triples(preds)</code>","text":"<p>Parse the triples into lists of entities and relations</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>List[Tuple[Span, RelationSpan, Span]]</code> <p>Predicted triples</p> required <p>Returns:</p> Type Description <code>Tuple[List[Span], List[RelationSpan]]</code> <p>Tuple with list of entities and list of relations</p>"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.predict","title":"<code>predict(docs, batch_size=None)</code>  <code>abstractmethod</code>","text":"<p>Perform the knowledge extraction.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[Span, RelationSpan, Span]]]</code> <p>the predicted triples</p>"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.set_device","title":"<code>set_device(device)</code>","text":"<p>Set the device to use</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, device]</code> required <p>Returns:</p> Type Description"},{"location":"knowledge_extractor/#zshot.KnowledgeExtractor.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Save component into disk</p>"},{"location":"mentions_extractor/","title":"MentionsExtractor","text":"<p>The mentions extractor will detect the possible entities (a.k.a. mentions), that will be then linked to a data source (e.g.: Wikidata) by the linker. </p> <p>Currently, there are 7 different mentions extractors supported, 2 of them are based on SpaCy, 2 of them are based on Flair, TARS, SMXM and GLiNER. The two different versions for SpaCy and Flair are similar, one is based on NERC and the other one is based on the linguistics (i.e.: using PoS and DP). The TARS and SMXM models can be used when the user wants to specify the mentions wanted to be extracted.</p> <p>The NERC approach will use NERC models to detect all the entities that have to be linked. This approach depends on the model that is being used, and the entities the model has been trained on, so depending on the use case and the target entities it may be not the best approach, as the entities may be not recognized by the NERC model and thus won't be linked.</p> <p>The linguistic approach relies on the idea that mentions will usually be a syntagma or a noun. Therefore, this approach detects nouns that are included in a syntagma and that act like objects, subjects, etc. This approach do not depend on the model (although the performance does), but a noun in a text should be always a noun, it doesn't depend on the dataset the model has been trained on.</p> <p>The SMXM model uses the description of the mentions to give the model information about them.</p> <p>TARS model will use the labels of the mentions to detect them.</p> <p>The GLiNER model will use the labels of the mentions to detect them.</p> <p>               Bases: <code>ABC</code></p>"},{"location":"mentions_extractor/#zshot.MentionsExtractor.extract_mentions","title":"<code>extract_mentions(docs, batch_size=None)</code>","text":"<p>Perform the mentions extraction. Call the predict function and add the mentions to the Spacy Doc</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description"},{"location":"mentions_extractor/#zshot.MentionsExtractor.load_models","title":"<code>load_models()</code>","text":"<p>Load the model</p> <p>Returns:</p> Type Description"},{"location":"mentions_extractor/#zshot.MentionsExtractor.predict","title":"<code>predict(docs, batch_size=None)</code>  <code>abstractmethod</code>","text":"<p>Perform the mentions prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code>"},{"location":"mentions_extractor/#zshot.MentionsExtractor.set_device","title":"<code>set_device(device)</code>","text":"<p>Set the device to use</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, device]</code> required <p>Returns:</p> Type Description"},{"location":"mentions_extractor/#zshot.MentionsExtractor.set_kg","title":"<code>set_kg(mentions)</code>","text":"<p>Set entities that mention extractor can use</p> <p>Parameters:</p> Name Type Description Default <code>mentions</code> <code>Iterator[Entity]</code> <p>The list of entities</p> required"},{"location":"regen/","title":"GENRE","text":"<p>Regen is based on GENRE. GENRE is also an entity linking model released by Facebook, but in this case it uses a different approach by conseidering the NERC task as a sequence-to-sequence problem, and retrieves the entities by using a constrained beam search to force the model to generate the entities.</p> <p>In a nutshell, (m)GENRE uses a sequence-to-sequence approach to entity retrieval (e.g., linking), based on fine-tuned BART. GENRE performs retrieval generating the unique entity name conditioned on the input text using constrained beam search to only generate valid identifiers. Although there is a version end-to-end of GENRE, it is not currently supported on ZShot (but it will). </p> <p>The REGEN linker will use the entities specified in the <code>zshot.PipelineConfig</code>.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>Linker</code></p> <p>REGEN linker</p> <p>Parameters:</p> Name Type Description Default <code>max_input_len</code> <p>Max length of input</p> <code>384</code> <code>max_output_len</code> <p>Max length of output</p> <code>15</code> <code>num_beams</code> <p>Number of beans to use</p> <code>10</code> <code>trie</code> <p>If the trie is given the linker will use it to restrict the search space. Custom entities won't be used if the trie is given.</p> <code>None</code>"},{"location":"regen/#zshot.linker.LinkerRegen.load_models","title":"<code>load_models()</code>","text":"<p>Load Model</p>"},{"location":"regen/#zshot.linker.LinkerRegen.load_tokenizer","title":"<code>load_tokenizer()</code>","text":"<p>Load Tokenizer</p>"},{"location":"regen/#zshot.linker.LinkerRegen.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"regen/#zshot.linker.LinkerRegen.restrict_decode_vocab","title":"<code>restrict_decode_vocab(_, prefix_beam)</code>","text":"<p>Restrict the possibilities of the Beam search to force the text generation</p>"},{"location":"regen/#zshot.linker.LinkerRegen.set_kg","title":"<code>set_kg(entities)</code>","text":"<p>Set new entities</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>Iterator[Entity]</code> <p>New entities to use</p> required"},{"location":"relation_extractor/","title":"Relations Extractor","text":"<p>The relations extractor will extract relations among different entities previously extracted by a linker.. </p> <p>Currently, the is only one Relation Extractor available: ZS-Bert</p> <p>               Bases: <code>ABC</code></p>"},{"location":"relation_extractor/#zshot.RelationsExtractor.extract_relations","title":"<code>extract_relations(docs, batch_size=None)</code>","text":"<p>Perform the relations extraction. Call the predict function and add the mentions to the Spacy Doc</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description"},{"location":"relation_extractor/#zshot.RelationsExtractor.load_models","title":"<code>load_models()</code>","text":"<p>Load the model</p> <p>Returns:</p> Type Description"},{"location":"relation_extractor/#zshot.RelationsExtractor.predict","title":"<code>predict(docs, batch_size=None)</code>  <code>abstractmethod</code>","text":"<p>Perform the relations extraction.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[RelationSpan]]</code> <p>the predicted relations</p>"},{"location":"relation_extractor/#zshot.RelationsExtractor.set_device","title":"<code>set_device(device)</code>","text":"<p>Set the device to use</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Union[str, device]</code> required <p>Returns:</p> Type Description"},{"location":"relation_extractor/#zshot.RelationsExtractor.set_relations","title":"<code>set_relations(relations)</code>","text":"<p>Set relationships that the relations extractor can use</p> <p>Parameters:</p> Name Type Description Default <code>relations</code> <code>Iterator[Relation]</code> <p>The list of relationship</p> required"},{"location":"relik_knowledge_extractor/","title":"ReLiK Knowledge Extractor","text":"<p>ReLiK is a lightweight and fast model for Entity Linking and Relation Extraction. It is composed of two main components: a retriever and a reader. The retriever is responsible for retrieving relevant documents from a large collection, while the reader is responsible for extracting entities and relations from the retrieved documents. </p> <p>In Zshot, we created a Knowledge Extractor to use ReLiK and extract relations directly, without having to specify any entities or relation names.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>KnowledgeExtractor</code></p> <p>Instantiate the KnowGL Knowledge Extractor</p>"},{"location":"relik_knowledge_extractor/#zshot.knowledge_extractor.KnowledgeExtractorRelik.load_models","title":"<code>load_models()</code>","text":"<p>Load relik model</p>"},{"location":"relik_knowledge_extractor/#zshot.knowledge_extractor.KnowledgeExtractorRelik.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Extract triples from docs</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Spacy Docs to process</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>Batch size for processing</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Tuple[Span, RelationSpan, Span]]]</code> <p>Triples (subject, relation, object) extracted for each document</p>"},{"location":"relik_linker/","title":"ReLiK Linker","text":"<p>ReLiK is a lightweight and fast model for Entity Linking and Relation Extraction. It is composed of two main components: a retriever and a reader. The retriever is responsible for retrieving relevant documents from a large collection, while the reader is responsible for extracting entities and relations from the retrieved documents. </p> <p>In Zshot, we created a linker to use ReLiK, and it works both providing entities or without providing entities, and with descriptions.</p> <p>This is an end-to-end model, so there is no need to use a mentions extractor before.</p> <p>The ReLiK linker will use the entities specified in the <code>zshot.PipelineConfig</code>, if any.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>Linker</code></p> <p>Relik linker</p>"},{"location":"relik_linker/#zshot.linker.LinkerRelik.is_end2end","title":"<code>is_end2end</code>  <code>property</code>","text":"<p>relik is end2end</p>"},{"location":"relik_linker/#zshot.linker.LinkerRelik.load_models","title":"<code>load_models()</code>","text":"<p>Load relik model</p>"},{"location":"relik_linker/#zshot.linker.LinkerRelik.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"smxm_linker/","title":"SMXM Linker","text":"<p>When there is no labelled data (i.e.: Zero-Shot approaches) the performance usually decreases due to the fact that the model doesn't really know what does the entity represent. To address this problem the SMXM model uses the description of the entities to give the model information about the entities.</p> <p>By using the descriptions, the SMXM model is able to understand the entity. Although this approach is Zero-Shot, as it doesn't need to have seen the entities during training, the user still have to specify the descriptions of the entities.</p> <p>This is an end-to-end model, so there is no need to use a mentions extractor before.</p> <p>The SMXM linker will use the entities specified in the <code>zshot.PipelineConfig</code>.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>Linker</code></p> <p>SMXM linker</p>"},{"location":"smxm_linker/#zshot.linker.LinkerSMXM.is_end2end","title":"<code>is_end2end</code>  <code>property</code>","text":"<p>SMXM is end2end model</p>"},{"location":"smxm_linker/#zshot.linker.LinkerSMXM.load_models","title":"<code>load_models()</code>","text":"<p>Load SMXM model</p>"},{"location":"smxm_linker/#zshot.linker.LinkerSMXM.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"smxm_mentions_extractor/","title":"SMXM Mentions Extractor","text":"<p>When the mentions to be extracted are known by the user they can be specified in order to improve the performance of the system. Based on the SMXM linker, the SMXM mentions extractor uses the description of the mentions to give the model information about the mentions to be extracted. By using the descriptions, the SMXM model is able to understand the mention.</p> <p>The SMXM mentions extractor will use the mentions specified in the <code>zshot.PipelineConfig</code>.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>MentionsExtractor</code></p> <p>SMXM Mentions Extractor</p>"},{"location":"smxm_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorSMXM.load_models","title":"<code>load_models()</code>","text":"<p>Load SMXM model</p>"},{"location":"smxm_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorSMXM.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"spacy_mentions_extractor/","title":"Spacy Mentions Extractor","text":"<p>               Bases: <code>MentionsExtractor</code></p> <p>SpaCy mentions extractor</p> <p>Parameters:</p> Name Type Description Default <code>extractor_type</code> <code>Optional[ExtractorType]</code> <p>Type of extractor to get mentions. One of: - NER: to use Named Entity Recognition model to get the mentions - POS: to get the mentions based on the linguistics</p> <code>NER</code>"},{"location":"spacy_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorSpacy.require_existing_ner","title":"<code>require_existing_ner</code>  <code>property</code>","text":"<p>If the type of the extractor is NER the existing NER is required</p>"},{"location":"spacy_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorSpacy.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Predict mentions of docs</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Documents to get mentions of</p> required <code>batch_size</code> <p>Batch size to use</p> <code>None</code> <p>Returns:</p> Type Description <p>Spans of the mentions</p>"},{"location":"spacy_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorSpacy.predict_ner_mentions","title":"<code>predict_ner_mentions(docs, batch_size=None)</code>","text":"<p>Predict mentions of docs using NER model</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Documents to get mentions of</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Batch size to use</p> <code>None</code> <p>Returns:</p> Type Description <p>Spans of the mentions</p>"},{"location":"spacy_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorSpacy.predict_pos_mentions","title":"<code>predict_pos_mentions(docs, batch_size=None)</code>","text":"<p>Predict mentions of docs using POS linguistics</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>Documents to get mentions of</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Batch size to use</p> <code>None</code> <p>Returns:</p> Type Description <p>Spans of the mentions</p>"},{"location":"tars_linker/","title":"TARS Linker","text":"<p>Task-aware representation of sentences (TARS), is a simple and effective method for few-shot and even zero-shot learning for text classification. However, it was extended to perform Zero-Shot NERC. </p> <p>Basically, TARS tries to convert the problem to a binary classification problem, predicting if a given text belongs to a specific class.</p> <p>TARS doesn't need the descriptions of the entities, so if you can't provide the descriptions of the entities maybe this is the approach you're looking for.</p> <p>The TARS linker will use the entities specified in the <code>zshot.PipelineConfig</code>.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>Linker</code></p> <p>TARS end2end Linker</p> <p>Parameters:</p> Name Type Description Default <code>default_entities</code> <code>Optional[str]</code> <p>Default entities to use in case no custom ones are set One of: - 'conll-short' - 'ontonotes-long' - 'ontonotes-short' - 'wnut_17-long' - 'wnut_17-short'</p> <code>'conll-short'</code>"},{"location":"tars_linker/#zshot.linker.LinkerTARS.flat_entities","title":"<code>flat_entities()</code>","text":"<p>As TARS use only the labels, take just the name of the entities and not the description</p>"},{"location":"tars_linker/#zshot.linker.LinkerTARS.load_models","title":"<code>load_models()</code>","text":"<p>Load TARS model if its not initialized</p>"},{"location":"tars_linker/#zshot.linker.LinkerTARS.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"tars_linker/#zshot.linker.LinkerTARS.set_kg","title":"<code>set_kg(entities)</code>","text":"<p>Set new entities in the model</p> <p>Parameters:</p> Name Type Description Default <code>entities</code> <code>Iterator[Entity]</code> <p>New entities to use</p> required"},{"location":"tars_mentions_extractor/","title":"TARS Mentions Extractor","text":"<p>When the mentions to be extracted are known by the user they can be specified in order to improve the performance of the system. Based on the TARS linker, the TARS mentions extractor uses the labels of the mentions to give the model information about the mentions to be extracted. TARS doesn't need the descriptions of the entities, so if you can't provide the descriptions of the entities maybe this is the approach you're looking for.</p> <p>The TARS mentions extractor will use the mentions specified in the <code>zshot.PipelineConfig</code>.</p> <ul> <li>Paper</li> <li>Original Source Code</li> </ul> <p>               Bases: <code>MentionsExtractor</code></p> <p>TARS end2end Linker</p> <p>Parameters:</p> Name Type Description Default <code>default_entities</code> <code>Optional[str]</code> <p>Default entities to use in case no custom ones are set One of: - 'conll-short' - 'ontonotes-long' - 'ontonotes-short' - 'wnut_17-long' - 'wnut_17-short'</p> <code>'conll-short'</code>"},{"location":"tars_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorTARS.flat_entities","title":"<code>flat_entities()</code>","text":"<p>As TARS use only the labels, take just the name of the entities and not the description</p>"},{"location":"tars_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorTARS.load_models","title":"<code>load_models()</code>","text":"<p>Load TARS model if its not initialized</p>"},{"location":"tars_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorTARS.predict","title":"<code>predict(docs, batch_size=None)</code>","text":"<p>Perform the entity prediction</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Iterator[Doc]</code> <p>A list of spacy Document</p> required <code>batch_size</code> <code>Optional[Union[int, None]]</code> <p>The batch size</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Span]]</code> <p>List Spans for each Document in docs</p>"},{"location":"tars_mentions_extractor/#zshot.mentions_extractor.MentionsExtractorTARS.set_kg","title":"<code>set_kg(mentions)</code>","text":"<p>Set new entities in the model</p> <p>Parameters:</p> Name Type Description Default <code>mentions</code> <code>Iterator[Entity]</code> <p>New entities to use</p> required"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#introduction","title":"Introduction","text":"<p>This is an introductory notebook to walk through all the functionalities of the ZShot plugin for Spacy.</p> <pre><code>from typing import Iterable\n\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy import displacy\n\nfrom zshot.utils.data_models import Entity, Span\nfrom zshot.mentions_extractor import (MentionsExtractor, MentionsExtractorSpacy, MentionsExtractorFlair, \n                                      MentionsExtractorTARS, MentionsExtractorSMXM)\nfrom zshot.mentions_extractor.utils import ExtractorType\nfrom zshot.linker import Linker, LinkerSMXM, LinkerTARS, LinkerBlink\nfrom zshot import PipelineConfig\n\nfrom datasets import Split\n</code></pre>"},{"location":"usage/#create-the-spacy-model-with-the-zshot-component","title":"Create the Spacy model with the  ZShot component","text":"<p>The first thing is to create the Spacy Model (a.k.a. <code>nlp</code>) with the ZShot component to perform zero-shot NERC.</p> <p>In order to do that, a <code>nlp</code> model has to be created first. Depending on the language, there are several models available:</p> <ul> <li><code>blank</code>: Spacy blank model. It has no trained pipelines.</li> <li><code>sm</code>: Spacy small model. It is faster than other but use to have less vocabulary and worse performance.</li> <li><code>md</code>: Spacy medium model. Slower than small models but with more vocabulary and better performance.</li> <li><code>lg</code>: Spacy large model. Slower than medium models but with more vocabulary and better performance.</li> <li><code>trf</code>: Spacy model based on Transformers. Slower that large models but with better performance.</li> </ul> <p>Note: For most of these models there will be also several options available, depending on the source of the data they were trained on</p> <p>Wich one should you use? Well, it depends on the mentions extractor and the linker you are going to use. If you rare going to use some Spacy-based mentions extractor you can't use the <code>blank</code> model. For the rest of mentions extractors and linker you can select the one that suits you the best.</p> <p>In this example the model based on transformers is going to be used.</p> <pre><code>nlp = spacy.load(\"en_core_web_trf\")\n</code></pre> <p>There are three main steps in order to create the ZShot component:  1. Add the <code>entities</code> to be extracted. They are zero-shot models not wizards, they don't know what you want.  2. Select the <code>mentions extractor</code> to use. The <code>mentions extractor</code> will extract broad mentions without a specific <code>entity</code> assigned.  3. Select the <code>linker</code> to use. The <code>linker</code> will link the mentions extracted by the <code>mentions extractor</code> and will link them to a specific <code>entity</code>. Some of the <code>linkers</code> are <code>end2end</code>, this is, they don't need a <code>mentions extractor</code> and therefore this field can be left empty.</p>"},{"location":"usage/#select-entities","title":"Select Entities","text":"<p>In order to specify the entities you can use the <code>Entity</code> class provided, that will have a label and a description. This description may be used by some linkers to improve the performance.</p> <pre><code>entities = [\n    Entity(name=\"company\", description=\"The name of a company\"),\n    Entity(name=\"location\", description=\"A physical location\"),\n    Entity(name=\"chemical compound\", description=\"Any of a large class of chemical compounds in \" \\\n           \"which one or more atoms of carbon are covalently linked to atoms of other elements, \" \\\n           \"most commonly hydrogen, oxygen, or nitrogen\")\n], \n</code></pre> <p>You can also use python <code>dict</code> (e.g.: loaded from a JSON).</p> <pre><code>entities = [\n    {\n        'name': \"company\", \n        'description': \"The name of a company\"\n    },\n    {\n        'name': \"location\", \n        'description': \"A physical location\"\n    },\n    {\n        'name': \"chemical compound\", \n        'description': \" Any of a large class of chemical compounds in \" \\\n           \"which one or more atoms of carbon are covalently linked to atoms of other elements, \" \\\n           \"most commonly hydrogen, oxygen, or nitrogen\"\n    }\n], \n</code></pre> <p>Or, if the <code>linker</code> you're going to use doesn't require the descriptions, you can use a <code>list</code> of strings containing the labels.</p> <pre><code>entities = [\n    \"company\",\n    \"location\",\n    \"chemical compound\"\n]\n</code></pre>"},{"location":"usage/#select-mention-extractor","title":"Select Mention Extractor","text":"<p>The <code>mentions_extractor</code> is the component that will extract broad mentions without a specific <code>entity</code> assigned. </p> <p>Currently, 4 different <code>mentions_extractor</code> are provided:</p> <ul> <li><code>MentionsExtractorSpacy</code></li> <li><code>MentionsExtractorFlair</code></li> <li><code>MentionsExtractorSMXM</code></li> <li><code>MentionsExtractorTARS</code></li> </ul> <p>To create a <code>mentions_extractor</code> just instantiate the class with the version to be used. There are two different versions for the SpaCy and Flair <code>mentions_extractor</code>:</p> <ul> <li>NER-Based: Will use a NER model to extract the mentions. </li> <li>POS-Based: Will use PoS tagging to extract the mentions.</li> </ul> <p>You can obtain them from the <code>ExtractorType</code>.</p> <pre><code># Using Spacy NER Mentions Extractor\nmentions_extractor = MentionsExtractorSpacy(ExtractorType.NER)\n# Using Spacy PoS Mentions Extractor\nmentions_extractor = MentionsExtractorSpacy(ExtractorType.POS)\n# Using Flair NER Mentions Extractor\nmentions_extractor = MentionsExtractorFlair(ExtractorType.NER)\n# Using Flair PoS Mentions Extractor\nmentions_extractor = MentionsExtractorFlair(ExtractorType.POS)\n</code></pre> <p>The <code>MentionsExtractorSMXM</code> will use descriptions of the mentions to extract them. Seethis</p> <p>The <code>MentionsExtractorTARS</code> will use the labels of the mentions to extract them. See this</p> <p>Both <code>MentionsExtractorSMXM</code> and <code>MentionsExtractorTARS</code> will use the mentions specified in the <code>zshot.PipelineConfig</code>, which is a list of <code>zshot.data_models.Entity</code>:</p> <pre><code>nlp = spacy.blank(\"en\")\n\nnlp_config = PipelineConfig(\n    mentions_extractor=MentionsExtractorSMXM(),\n    mentions=[\n        Entity(name=\"company\", description=\"The name of a company\"),\n        Entity(name=\"location\", description=\"A physical location\"),\n        Entity(name=\"chemical compound\", description=\"Any of a large class of chemical compounds in which one or more atoms of carbon are covalently linked to atoms of other elements, most commonly hydrogen, oxygen, or nitrogen\")\n    ]\n)\nnlp.add_pipe(\"zshot\", config=nlp_config, last=True)\n</code></pre>"},{"location":"usage/#select-linker","title":"Select Linker","text":"<p>The <code>linker</code> is the component that will link the extracted mentions to a specific <code>entity</code>. Some of them are <code>end2end</code>, this is, they don't need and won't use the <code>mentions_extractor</code>. </p> <p>Currently, 4 different <code>linker</code> are provided:</p> <ul> <li><code>LinkerBLINK</code>: See this</li> <li><code>LinkerRegen</code> See this</li> <li><code>LinkerSMXM</code>: <code>end2end</code> model that uses descriptions. See this</li> <li><code>LinkerTARS</code>: <code>end2end</code> model. See this</li> </ul> <pre><code>linker = LinkerTARS()\n</code></pre>"},{"location":"usage/#create-the-pipeline-config","title":"Create the Pipeline Config","text":"<p>Once that the <code>entities</code>, the <code>mentions_extractor</code> and the <code>linker</code> are selected, the <code>PipelineConfig</code> can be created to configure the ZShot component.</p> <pre><code>config = PipelineConfig(\n    entities=entities,\n    mentions_extractor=mentions_extractor,\n    linker=linker\n)\n</code></pre> <p>Or you can create everything on the fly:</p> <pre><code>config = PipelineConfig(\n    entities=[\n        Entity(name=\"company\", description=\"The name of a company\"),\n        Entity(name=\"location\", description=\"A physical location\"),\n        Entity(name=\"chemical compound\", description=\"Any of a large class of chemical compounds in which one or more atoms of carbon are covalently linked to atoms of other elements, most commonly hydrogen, oxygen, or nitrogen\")\n    ], \n    linker=LinkerSMXM()\n)\n</code></pre>"},{"location":"usage/#create-the-component","title":"Create the component","text":"<p>Once the <code>PipelineConfg</code> has been created it's time to create the ZShot component and add it to the <code>nlp</code> pipe. Use the <code>last=True</code> option to assure the model is added to the end of the pipe, as some components have to be executed first.</p> <p><pre><code>nlp.add_pipe(\"zshot\", config=config, last=True)\n</code></pre>     WARNING:root:Disabling default NER"},{"location":"usage/#execute","title":"Execute","text":"<p>Now you can use the <code>nlp</code> model as always to see the entities extracted!</p> <pre><code>text_acetamide = \"CH2O2 is a chemical compound similar to Acetamide used in International Business \" \\\n        \"Machines Corporation (IBM) to create new materials that act like PAGs.\"\n\ndoc = nlp(text_acetamide)\ndisplacy.render(doc, style=\"ent\")\n</code></pre> <p>      CH2O2     chemical compound   is a chemical compound similar to       Acetamide     chemical compound   used in       International Business Machines Corporation     company   (      IBM     company  ) to create new materials that act like PAGs.</p> <pre><code>for ent in doc.ents:\n    print(ent.text, \"-\", ent.label_)\n</code></pre> <pre><code>CH2O2 - chemical compound\nAcetamide - chemical compound\nInternational Business Machines Corporation - company\nIBM - company\n</code></pre>"},{"location":"usage/#use-our-display","title":"Use our display","text":"<p>If you don't like the gray color of displacy, or you want different colors for each entity, you can use our displacy tool</p> <pre><code>from zshot import displacy\n</code></pre> <pre><code>text_acetamide = \"CH2O2 is a chemical compound similar to Acetamide used in International Business \" \\\n        \"Machines Corporation (IBM) to create new materials that act like PAGs.\"\n\ndoc = nlp(text_acetamide)\ndisplacy.render(doc, style=\"ent\")\n</code></pre> <p>      CH2O2     chemical compound   is a chemical compound similar to       Acetamide     chemical compound   used in       International Business Machines Corporation     company   (      IBM     company  ) to create new materials that act like PAGs.</p>"},{"location":"usage/#use-your-own-component","title":"Use your own component","text":"<p>If you want to implement your own <code>mentions_extractor</code> or <code>linker</code> and use it with ZShot you can do it. To make it easier for the user to implement a new component, some base classes are provided that you have to extend with your code.</p> <p>It is as simple as create a new class extending the base class (<code>MentionsExtractor</code> or <code>Linker</code>). You will have to implement the predict method, which will receive the Spacy Documents and will return a list of <code>zshot.utils.data_models.Span</code> for each document.</p> <p>Let's create a simple <code>mentions_extractor</code> that will extract as mentions all words that contain the letter s:</p> <pre><code>class SimpleMentionExtractor(MentionsExtractor):\n    def predict(self, docs: Iterable[Doc], batch_size=None):\n        spans = [[Span(tok.idx, tok.idx + len(tok)) for tok in doc if \"s\" in tok.text] for doc in docs]\n        return spans\n</code></pre> <p>Now, let's create a new <code>nlp</code> model with a ZShot component with the new <code>mentions_extractor</code></p> <p><pre><code>new_nlp = spacy.load(\"en_core_web_trf\")\n\nconfig = PipelineConfig(\n    mentions_extractor=SimpleMentionExtractor()\n)\nnew_nlp.add_pipe(\"zshot\", config=config, last=True)\n</code></pre>     WARNING:root:Disabling default NER      <p>And let's try it:</p> <pre><code>text_acetamide = \"CH2O2 is a chemical compound similar to Acetamide used in International Business \" \\\n        \"Machines Corporation (IBM) to create new materials that act like PAGs.\"\n\ndoc = new_nlp(text_acetamide)\nprint(doc._.mentions)\n</code></pre> <pre><code>[is, similar, used, Business, Machines, materials, PAGs]\n</code></pre>"},{"location":"usage/#evaluation","title":"Evaluation","text":"<p>If you have a new ZShot component maybe you want to evaluate it over some famous benchmarks to get an idea of the performance of your model.</p> <p>ZShot evaluation package contains all you need to do it. It makes it easy for the user to evaluate the component over a Zero-Shot dataset.</p> <p>The list of the datasets available at the moment is:  - OntoNotes. See this  - MedMentions. See this</p> <p>Now you can use the <code>evaluate</code> function to evaluate your <code>nlp</code> over a dataset.</p> <p>You can evaluate one or more dataset, and using just one or more splits.</p> <pre><code>def evaluate(nlp: spacy.Language,\n             datasets: Union[str, List[str]],\n             splits: Optional[Union[str, List[str]]] = None) -&gt; str:\n    \"\"\" Evaluate a spacy zshot model\n\n    :param nlp: Spacy Language pipeline with ZShot components\n    :param datasets: Dataset or list of datasets to evaluate\n    :param splits: Optional. Split or list of splits to evaluate. All splits available by default\n    :return: Result of the evaluation. String containing a table with the result\n    \"\"\"\n</code></pre> <pre><code>from zshot.evaluation.zshot_evaluate import evaluate\nfrom datasets import Split\n\nevaluation = evaluate(new_nlp, \"ontonotes\", \n                      splits=[Split.VALIDATION])\nprint(evaluation)\n</code></pre>"},{"location":"zsbert_relations_extractor/","title":"ZS-BERT Relations Extractor","text":"<p>The ZS-BERT model is a novel multi-task learning model to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their relations, ZS-BERT learns two functions that project sentences and relation descriptions into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations.</p> <p>This <code>RelationsExtractor</code> uses relations pre-defined along with their descriptions, added into the <code>PipelineConfig</code> using the <code>Relation</code> data model.</p> <ul> <li>Paper</li> <li>Original Repo</li> </ul> <p>               Bases: <code>RelationsExtractor</code></p>"}]}